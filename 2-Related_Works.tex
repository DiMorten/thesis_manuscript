\chapter{RELATED WORK}\label{sect:RelatetWorks}

Recent works have used fully convolutional neural networks (CNNs) for deforestation mapping \cite{ortega2021comparison, adarme2022multi, torres2021deforestation}. In those works, the input to the CNNs was the concatenation of the optical images acquired at different dates, i.e., $T_{-1}$ and $T_{0}$, in a so-called early fusion scheme. In \cite{ortega2021comparison}, an encoder-decoder fully convolutional network model based on the ResUnet \cite{jha2019resunet++} delivered the best results when compared to different CNN architectures used for deforestation detection in the Brazilian Amazon. 

Uncertainty in machine learning models can be divided into aleatoric or data uncertainty and epistemic or model uncertainty. Data uncertainty describes the confidence of the data and it is related to the inherent randomness of the input data. Data uncertainty cannot be reduced by increasing the amount of training samples. Model uncertainty describes the confidence of the prediction and it can be reduced by collecting more training data.


% is related to the inability of the model to produce correct outcomes
% it is high when missing training data

Various approaches have been proposed to estimate uncertainty in deep learning models. The first group are confidence-based approaches, which estimate uncertainty directly on the outcome of a single inference run. Multiple works used confidence-based approaches for uncertainty estimation \cite{amorocho1973entropy, wang2014new, zhan2022comparative}. In \cite{wang2014new}, a comparison of confidence-based approaches was made for entropy, maximum margin and least confidence in an active learning setting, where the three approaches performed similarly. 

Alternatively, multiple-outcome uncertainty estimation methods rely on either multiple training runs or multiple inference runs to estimate the uncertainty related to a neural network's weights. These approaches measure the level of disagreement among multiple outcomes for the same input. 

% Multiple-outcome methods estimate the epistemic uncertainty.

% These approaches can only estimate data uncertainty.

The first multiple-outcome approach trains an ensemble of networks and computes statistical measures that consider the different predictions of the individual networks that compose the ensemble \cite{lakshminarayanan2017simple, mehrtash2020confidence}. Training multiple models is, however, computationally expensive. 

Bayesian networks learn the posterior distribution for a network's trainable weights, allowing to compute the principled predictive uncertainty. However, Bayesian techniques have been proven to be unpractical for deep neural networks due to the large amount of data needed, proportional to the number of network parameters \cite{gawlikowski2021survey}. 

The most common approximation for Bayesian networks is Monte Carlo Dropout (MCD) \cite{gal2016dropout}. Dropout is commonly used during training as a regularization technique. MCD additionally uses dropout at inference, producing a different outcome in each inference run. Uncertainty is then estimated by calculating statistical measures like variance and entropy over a predefined number of inference runs.

% Multiple works have used MCD in classification problems \cite{}.
% The most widely adopted approximation of Bayesian networks is MCD. Multiple works have used MCD in classification problems 
% have also been used for uncertainty estimation. Bayesian networks learn the probability distribution for all trainable weights. 
% Each inference run will draw a different outcome from the learned probability distributions. Uncertainty is estimated as a statistical measure applied to multiple inference runs. 
Recently, various works have used MCD for uncertainty estimation in semantic segmentation applications that rely on deep learning models. Many of those works used U-Net \cite{ronneberger2015u} based networks, and employed MCD at inference time to approximate a Bayesian network. Such an approach has been employed in many application areas such as urban mapping with aerial and satellite images \cite{dechesne2021bayesian}, medical image segmentation \cite{nguyen2021comparison, kwon2020uncertainty}, and fingerprint ROI segmentation \cite{joshi2021explainable}. MCD was also used in \cite{huang2018efficient} to estimate uncertainty in the semantic segmentation of video frames. In all the above-mentioned works, however, the estimated uncertainty maps were only used in the analysis of the corresponding models' predictions, i.e., they were not employed in further processing steps.%, and they did not propose any further procedure to take advantage of uncertainty in the auditing procedure. 

In \cite{wu2022closer}, the resulting uncertainty map estimated using MCD was used as an additional input to train a second U-Net network. The map was concatenated with the original input image, for the semantic segmentation of scanned historical maps. % Although that work used uncertainty to improve the classification results, it did not involve any human auditing based on the uncertainty maps. 
Multiple-outcome methods are computationally expensive because they require to either train or infer multiple times. Evidential deep learning \cite{sensoy2018evidential} is an alternative which estimates model uncertainty with a single training and inference run. It assumes that the outcome of a single deterministic network is a subjective opinion and learn the function leading to those opinions as a Dirichlet distribution, by directly estimating the Dirichlet parameter $\alpha$ at the output of the network. Their work performed similarly to multiple-outcome approaches while being less computationally expensive. 

Recent works have used evidential learning for semantic segmentation \cite{do2022epistemic, holmquist2023evidential, tong2021evidential}. In \cite{do2022epistemic}, a FCN is used for evidential learning by replacing the softmax layer with ReLU and attaching a Dirichlet layer at the outcome, which produced the Dirichlet parameters $\alpha$ at the output. They estimated uncertainty for the semantic segmentation of underwater imagery. Their approach was the same as used in this work, although the application field was a different one. Although they qualitatively observed a correspondence between low accuracy and high uncertainty, they did not quantify the benefits of using uncertainty. In \cite{holmquist2023evidential}, evidential learning was used for class incremental learning, where a previously trained network is extended with new classes. The authors modeled the occurrence of an unknown class (background) as the estimated uncertainty. The proposed approach outperformed other state-of-the-art methods for incremental learning. In \cite{tong2021evidential}, 


In a way similar to our work, multiple methods have used confidence-based approaches, MCD, ensembles and evidential learning for uncertainty estimation in an active learning scheme, although they were devised for different application fields \cite{zhdanov2019diverse, di2019deep, gal2017deep, kirsch2019batchbald, aghdam2019active, ren2021survey, yang2017suggestive, hemmer2022deal}. In \cite{zhdanov2019diverse}, a confidence-based approach (maximum margin) was used to select the most relevant samples in Natural Language Processing (NLP) and image classification tasks. The authors did not assess other uncertainty estimation methods. In \cite{di2019deep}, the authors inferred on test data, estimated the uncertainty maps, and the samples with highest uncertainty were sent to an expert who manually annotated them. Then the network was re-trained and the cycle was repeated for multiple iterations. That work was employed in the semantic segmentation of histology data. Similarly, \cite{gal2017deep} used MCD to compute uncertainty metrics for active learning, in the context of image classification; the proposed method was evaluated on the MNIST dataset and biomedical data. In \cite{aghdam2019active}, MCD was used to compute a guiding metric for active learning in object detection tasks, by annotating and fine-tuning the network with the highest ranked test samples according to their uncertainty scores. They evaluated the method using pedestrian detection datasets. In \cite{yang2017suggestive}, ensembles were used for selecting images with the highest uncertainty to be re-annotated in an active learning scheme for biomedical image segmentation. Different from our work, they selected a specific number of images with the highest uncertainty for an auditor to annotate. Instead, we select image regions within a remote sensing raster for an auditor to re-annotate. In \cite{hemmer2022deal}, evidential deep learning was used for active learning. Their approach outperformed multiple-outcome approaches like MCD and ensembles. However, they only did experiments with image classification. Instead, our work estimates uncertainty for semantic segmentation. 

In \cite{zhan2022comparative}, a comparison of multiple uncertainty estimation techniques including confidence-based methods, MCD and ensembles was made for active learning in image classification tasks. They found that none of the methods performed significantly better than the others.


\textit{Works using uncertainty in transformers}

Uncertainty has been used with transformer networks working with NLP tasks instead of images. In \cite{pei2022transformer}, uncertainty was estimated using Monte Carlo Droput (MCD), ensembles and a novel uncertainty estimation method for a transformer network in a NLP task. Similarly, \cite{sankararaman2022bayesformer} made modifications to the transformer architecture by applying independent dropout masks to key, query and value vector representations and estimates uncertainty using MCD in language classification tasks. The authors in \cite{vazhentsev2022uncertainty} compared multiple uncertainty estimation techniques including MCD, ensembles and simpler ones like the Mahalanobis distance in a transformer network for NLP tasks. They found that simple methods like Mahalanobis distance performed on par compared to more computationally expensive methods like MCD and ensembles.

Recently, vision transformers have proved to be a relevant alternative to traditional fully convolutional networks for semantic segmentation. In \cite{ranftl2021vision}, the authors used an encoder-decoder architecture where the encoder consisted of a sequence of vision transformers (ViT) \cite{dosovitskiy2020image}, while the decoder consisted of convolution and upsampling operations. However, no works have been published for uncertainty estimation in vision tranformers for semantic segmentation. In \cite{yang2021uncertainty}, the authors estimated uncertainty using a separate Fully Convolutional Network (FCN) called uncertainty quantification network, and then used the uncertainty outcomes as a guiding input for ViT. However, they did not estimate the ViT uncertainty and their work did not consider semantic segmentation. In \cite{bin2022efficient} the Multi-Head Attention (MHA) from ViT was replaced by Fourier attention consisting of a 2D fast Fourier transform \cite{lee2021fnet}, which was more computationally efficient and presented comparative effectiveness to MHA. Uncertainty was estimated using MCD on the damage assessment head, which was composed by convolution operations. They applied their method for semantic segmentation of disaster assessment in aerial images. However, uncertainty was not measured at the attention stage.

